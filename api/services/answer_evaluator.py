"""
LLM-based answer evaluation service
Uses the LLM to semantically evaluate answer correctness and provide confidence scores
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from typing import Dict, Optional, Tuple
from llm.llm_factory import get_llm_client
from llm.llm_client import Chunk, Citation


class AnswerEvaluator:
    """Service for evaluating answer correctness using LLM"""
    
    def __init__(self):
        self.llm_client = None  # Lazy load
    
    def _get_llm_client(self):
        """Lazy load LLM client"""
        if self.llm_client is None:
            self.llm_client = get_llm_client()
        return self.llm_client
    
    async def evaluate_answer(
        self,
        question: str,
        actual_answer: str,
        expected_answer: Optional[str] = None,
        context_chunks: Optional[list] = None
    ) -> Dict:
        """
        Evaluate answer correctness using LLM semantic comparison
        
        Args:
            question: The question that was asked
            actual_answer: The answer generated by the system
            expected_answer: Optional expected answer for comparison
            context_chunks: Optional context chunks used to generate the answer
            
        Returns:
            Dict with:
                - confidence: float (0.0 to 1.0)
                - correctness: bool
                - reasoning: str (optional explanation)
        """
        import logging
        logger = logging.getLogger(__name__)
        
        # If no expected answer, evaluate based on answer quality alone
        if not expected_answer:
            return await self.evaluate_answer_quality(question, actual_answer, context_chunks)
        
        # Use LLM to compare actual vs expected
        return await self._evaluate_answer_comparison(question, actual_answer, expected_answer, context_chunks)
    
    async def evaluate_answer_quality(
        self,
        question: str,
        answer: str,
        context_chunks: Optional[list] = None
    ) -> Dict:
        """Evaluate answer quality without expected answer"""
        llm_client = self._get_llm_client()
        
        # Build context summary if available
        context_summary = ""
        if context_chunks:
            context_summary = f"\n\nContext used: {len(context_chunks)} chunks from documents"
        
        evaluation_prompt = f"""Evaluate the quality and correctness of this answer to a question.

Question: {question}

Answer: {answer}
{context_summary}

Evaluate:
1. Does the answer directly address the question?
2. Is the answer complete and informative?
3. Does the answer appear to be based on factual information (not made up)?

Respond with a JSON object:
{{
    "confidence": <float between 0.0 and 1.0>,
    "correctness": <true or false>,
    "reasoning": "<brief explanation>"
}}

Confidence should reflect:
- 0.9-1.0: Answer is highly confident and directly addresses the question
- 0.7-0.9: Answer is good but may have minor issues
- 0.5-0.7: Answer is partially correct or incomplete
- 0.0-0.5: Answer is incorrect, incomplete, or doesn't address the question

Return ONLY the JSON object, no other text."""

        try:
            # Use a simple context for evaluation
            dummy_chunks = [Chunk(
                text=evaluation_prompt,
                doc_id="evaluation",
                clause_number=None,
                page_num=0,
                span_start=0,
                span_end=0,
                source_uri="evaluation"
            )]
            
            result = await llm_client.generate_answer(
                query="Evaluate this answer",
                context_chunks=dummy_chunks,
                citations=[]
            )
            
            # Parse JSON from result
            import json
            import re
            
            # Extract JSON from response
            json_match = re.search(r'\{[^}]+\}', result.text, re.DOTALL)
            if json_match:
                eval_result = json.loads(json_match.group())
                return {
                    "confidence": float(eval_result.get("confidence", 0.5)),
                    "correctness": bool(eval_result.get("correctness", False)),
                    "reasoning": eval_result.get("reasoning", "")
                }
            else:
                # Fallback: try to extract confidence from text
                confidence_match = re.search(r'confidence["\']?\s*:\s*([0-9.]+)', result.text, re.IGNORECASE)
                confidence = float(confidence_match.group(1)) if confidence_match else 0.5
                return {
                    "confidence": confidence,
                    "correctness": confidence >= 0.7,
                    "reasoning": result.text[:200]
                }
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"Error evaluating answer quality: {e}")
            # Fallback to basic heuristic
            if answer and len(answer) > 10 and "cannot find" not in answer.lower():
                return {"confidence": 0.6, "correctness": False, "reasoning": "Evaluation failed, using fallback"}
            else:
                return {"confidence": 0.2, "correctness": False, "reasoning": "Answer appears incomplete or missing"}
    
    async def _evaluate_answer_comparison(
        self,
        question: str,
        actual_answer: str,
        expected_answer: str,
        context_chunks: Optional[list] = None
    ) -> Dict:
        """Compare actual answer to expected answer using LLM"""
        llm_client = self._get_llm_client()
        import logging
        logger = logging.getLogger(__name__)
        
        # Build context summary if available
        context_summary = ""
        if context_chunks:
            context_summary = f"\n\nContext used: {len(context_chunks)} chunks from documents"
        
        evaluation_prompt = f"""Compare two answers to determine if they are semantically equivalent and evaluate correctness.

Question: {question}

Expected Answer: {expected_answer}

Actual Answer: {actual_answer}
{context_summary}

Evaluate:
1. Are the answers semantically equivalent (do they convey the same meaning)?
2. Does the actual answer correctly address the question?
3. Is the actual answer complete and accurate?

For clause questions: The actual answer may contain more detail than the expected answer. This is acceptable if the actual answer correctly describes what the clause specifies.

Respond with a JSON object:
{{
    "confidence": <float between 0.0 and 1.0>,
    "correctness": <true or false>,
    "reasoning": "<brief explanation>"
}}

Confidence should reflect:
- 0.9-1.0: Answers are semantically equivalent or actual answer is more detailed but correct
- 0.7-0.9: Answers are mostly equivalent with minor differences
- 0.5-0.7: Answers are partially equivalent or actual answer is incomplete
- 0.0-0.5: Answers are not equivalent or actual answer is incorrect

Return ONLY the JSON object, no other text."""

        try:
            # Use a simple context for evaluation
            dummy_chunks = [Chunk(
                text=evaluation_prompt,
                doc_id="evaluation",
                clause_number=None,
                page_num=0,
                span_start=0,
                span_end=0,
                source_uri="evaluation"
            )]
            
            result = await llm_client.generate_answer(
                query="Compare these answers",
                context_chunks=dummy_chunks,
                citations=[]
            )
            
            # Parse JSON from result
            import json
            import re
            
            # Extract JSON from response
            json_match = re.search(r'\{[^}]+\}', result.text, re.DOTALL)
            if json_match:
                eval_result = json.loads(json_match.group())
                return {
                    "confidence": float(eval_result.get("confidence", 0.5)),
                    "correctness": bool(eval_result.get("correctness", False)),
                    "reasoning": eval_result.get("reasoning", "")
                }
            else:
                # Fallback: try to extract confidence from text
                confidence_match = re.search(r'confidence["\']?\s*:\s*([0-9.]+)', result.text, re.IGNORECASE)
                confidence = float(confidence_match.group(1)) if confidence_match else 0.5
                return {
                    "confidence": confidence,
                    "correctness": confidence >= 0.7,
                    "reasoning": result.text[:200]
                }
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"Error evaluating answer comparison: {e}")
            # Fallback to basic string matching
            return self._fallback_evaluation(actual_answer, expected_answer)
    
    def _fallback_evaluation(self, actual_answer: str, expected_answer: str) -> Dict:
        """Fallback evaluation using string matching"""
        actual_lower = actual_answer.lower().strip()
        expected_lower = expected_answer.lower().strip()
        
        if actual_lower == expected_lower:
            return {"confidence": 1.0, "correctness": True, "reasoning": "Exact match"}
        elif expected_lower in actual_lower or actual_lower in expected_lower:
            return {"confidence": 0.9, "correctness": True, "reasoning": "Substring match"}
        else:
            # Word overlap
            expected_words = set(expected_lower.split())
            actual_words = set(actual_lower.split())
            if expected_words and actual_words:
                overlap = len(expected_words & actual_words)
                confidence = overlap / max(len(expected_words), len(actual_words))
                return {
                    "confidence": confidence,
                    "correctness": confidence >= 0.7,
                    "reasoning": f"Word overlap: {overlap}/{max(len(expected_words), len(actual_words))}"
                }
        
        return {"confidence": 0.3, "correctness": False, "reasoning": "Low similarity"}


# Global instance
answer_evaluator = AnswerEvaluator()

