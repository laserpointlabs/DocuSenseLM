# Model Comparison Test Report

**Generated:** 2025-11-07T14:14:35.978511

## Executive Summary

| Model | Pass Rate | Avg Confidence | Avg Time (ms) | Avg Gen Time (ms) | Avg Eval Time (ms) |
|-------|-----------|----------------|---------------|-------------------|-------------------|
| **mistral:7b** | 92.0% (46/50) | 0.926 | 3151 | 1727 | 1423 |
| **granite3.3:8b (thinking)** | 88.0% (44/50) | 0.847 | 3749 | 2026 | 1722 |
| **granite3.3:8b** | 86.0% (43/50) | 0.840 | 3964 | 2149 | 1814 |
| **llama3.2:3b** | 82.0% (41/50) | 0.808 | 2317 | 1277 | 1039 |

## Detailed Performance Metrics

### Correctness Comparison


#### mistral:7b
- **Pass Rate:** 92.0% (46/50)
- **Failed:** 4/50
- **Average Confidence:** 0.926

**Category Breakdown:**
- Effective Date: 77.8% (7/9)
- Expiration Date: 88.9% (8/9)
- Governing Law: 88.9% (8/9)
- Location: 100.0% (2/2)
- Mutual Status: 100.0% (6/6)
- Parties: 100.0% (6/6)
- Term: 100.0% (9/9)

#### granite3.3:8b (thinking)
- **Pass Rate:** 88.0% (44/50)
- **Failed:** 6/50
- **Average Confidence:** 0.847

**Category Breakdown:**
- Effective Date: 66.7% (6/9)
- Expiration Date: 77.8% (7/9)
- Governing Law: 88.9% (8/9)
- Location: 100.0% (2/2)
- Mutual Status: 100.0% (6/6)
- Parties: 100.0% (6/6)
- Term: 100.0% (9/9)

#### granite3.3:8b
- **Pass Rate:** 86.0% (43/50)
- **Failed:** 7/50
- **Average Confidence:** 0.840

**Category Breakdown:**
- Effective Date: 66.7% (6/9)
- Expiration Date: 66.7% (6/9)
- Governing Law: 88.9% (8/9)
- Location: 100.0% (2/2)
- Mutual Status: 100.0% (6/6)
- Parties: 100.0% (6/6)
- Term: 100.0% (9/9)

#### llama3.2:3b
- **Pass Rate:** 82.0% (41/50)
- **Failed:** 9/50
- **Average Confidence:** 0.808

**Category Breakdown:**
- Effective Date: 66.7% (6/9)
- Expiration Date: 100.0% (9/9)
- Governing Law: 88.9% (8/9)
- Location: 100.0% (2/2)
- Mutual Status: 83.3% (5/6)
- Parties: 100.0% (6/6)
- Term: 55.6% (5/9)


### Speed Comparison

| Model | Avg Total Time | Avg Generation | Avg Evaluation | Speed Rank |
|-------|----------------|----------------|----------------|------------|
| llama3.2:3b | 2317ms | 1277ms | 1039ms | #1 |
| mistral:7b | 3151ms | 1727ms | 1423ms | #2 |
| granite3.3:8b (thinking) | 3749ms | 2026ms | 1722ms | #3 |
| granite3.3:8b | 3964ms | 2149ms | 1814ms | #4 |

### Confidence Score Comparison

| Model | Avg Confidence | Confidence Rank |
|-------|----------------|-----------------|
| mistral:7b | 0.926 | #1 |
| granite3.3:8b (thinking) | 0.847 | #2 |
| granite3.3:8b | 0.840 | #3 |
| llama3.2:3b | 0.808 | #4 |

## Overall Rankings

### Best Overall Performance (Pass Rate + Confidence)
1. **mistral:7b** (Score: 1.000)
2. **granite3.3:8b (thinking)** (Score: 0.940)
3. **granite3.3:8b** (Score: 0.924)
4. **llama3.2:3b** (Score: 0.884)


### Fastest Model
1. **llama3.2:3b** (2317ms avg)

### Highest Confidence
1. **mistral:7b** (0.926 avg)

## Recommendations


- **Best Overall:** mistral:7b - Best balance of correctness and confidence (92% pass rate, 0.926 confidence)
- **Fastest:** llama3.2:3b - Best for low-latency applications (2317ms avg)
- **Most Confident:** mistral:7b - Best for high-confidence requirements (0.926 avg)
- **Best with Thinking Mode:** granite3.3:8b (thinking) - Improved reasoning for complex queries (88% pass rate, faster than standard Granite)

### Model-Specific Notes


#### Granite 3.3:8b
- Supports 128K context window (tested with 32K for comparison)
- Has "thinking" mode capability for improved reasoning
- Optimized for RAG and instruction-following tasks
- See: https://ollama.com/library/granite3.3

#### Granite 3.3:8b (Thinking Mode)
- **Thinking Mode Enabled:** Uses `/api/chat` endpoint with control message `{"role": "control", "content": "thinking"}`
- **Performance Improvement:** 
  - Pass rate improved from 86% to 88% (+2 percentage points)
  - Response time improved from 3964ms to 3749ms (-215ms, ~5% faster)
  - Confidence improved from 0.840 to 0.847
- **Best Use Cases:** Complex reasoning tasks, multi-step problem solving
- **Implementation:** Automatically enabled when `ENABLE_THINKING=true` environment variable is set for Granite models


## Test Configuration

- **Total Questions:** 50
- **Models Tested:** llama3.2:3b, mistral:7b, granite3.3:8b, granite3.3:8b (thinking)
- **Evaluation Method:** LLM-based semantic evaluation
- **Context Length:** 8K-32K (model-dependent)

## Raw Results

Full detailed results are available in the JSON output file.

---
*Report generated by Model Comparison Test Suite*
